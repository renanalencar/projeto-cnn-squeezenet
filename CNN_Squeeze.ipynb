{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Projeto_CNN_Squeeze_v5.0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qsSxQJGtbqY0",
        "-mNIpyhd1b2b",
        "J95g7BvAAXeT",
        "KfpHFSrIAaaR",
        "6z1np-1Xb-ps",
        "ncVIknOucMT7",
        "CGynoYsq1jAc",
        "xK9aTjyu1m76",
        "M0omRck61pfq",
        "p0jvliX61vJa",
        "tHY-ZdxkJ9aZ",
        "Yv_iQWYH1zET",
        "Z3F4JZxd5wrJ",
        "OA_-LHzfIdVq",
        "TX0otG_AJE9y",
        "4KcKjO9u53Er",
        "nn_sw3kN55aE",
        "4LZ7i2pf58i9",
        "ZIB-x4IIH_J9"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TF_Ahcf1Wpk"
      },
      "source": [
        "# Projeto de Redes Neurais "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OTE95hcbO3U"
      },
      "source": [
        "## Referências: \n",
        "\n",
        "1. [Estudo sobre Câncer de Cólon utilizando a mesma base com modelod e CNN](https://www.kaggle.com/aayushrajput/lung-colon-cancer)\n",
        "2. [Modelo de SqueezeNet para reconhecimento de comida](https://www.kaggle.com/kmader/food-squeezenet) \n",
        "3. [SqueezeNet no Keras](https://codelabs.developers.google.com/codelabs/keras-flowers-squeezenet#6)\n",
        "4. [Entendendo Redes Convolucionais (CNNs) ](https://medium.com/neuronio-br/entendendo-redes-convolucionais-cnns-d10359f21184#:~:text=H%C3%A1%20muitas%20fun%C3%A7%C3%B5es%2C%20como%20sigmoid,quando%20comparada%20a%20outras%20fun%C3%A7%C3%B5es)\n",
        "5. [Uma introdução as redes neurais convolucionais utilizando o Keras](https://medium.com/data-hackers/uma-introdu%C3%A7%C3%A3o-as-redes-neurais-convolucionais-utilizando-o-keras-41ee8dcc033e)\n",
        "6. [Tutorial prático do Keras](https://cv-tricks.com/tensorflow-tutorial/keras/)\n",
        "7. [Métricas de avaliação de modelo](https://gabrielschade.github.io/2019/03/12/ml-classificacao-metricas.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsSxQJGtbqY0"
      },
      "source": [
        "## Erro na biblioteca keras_applications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iGEAqxx2l_z"
      },
      "source": [
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mNIpyhd1b2b"
      },
      "source": [
        "## Importanto as bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-06T02:26:25.508587Z",
          "iopub.status.busy": "2021-01-06T02:26:25.507936Z",
          "iopub.status.idle": "2021-01-06T02:26:32.576008Z",
          "shell.execute_reply": "2021-01-06T02:26:32.576483Z"
        },
        "id": "iAve6DCL4JH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a058cebb-143e-47fc-8b79-1f4e048f398e"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "from keras import backend as K\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.applications.vgg16 import VGG16R\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense,Flatten\n",
        "from keras.layers import Input, Activation, concatenate\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.layers import Dense,Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D,Conv2D\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import get_file\n",
        "from keras.utils import layer_utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from visual_callbacks import AccLossPlotter\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MazPTJD92222"
      },
      "source": [
        "## Pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J95g7BvAAXeT"
      },
      "source": [
        "### Variáveiras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCmmex5F3MEb"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/lung_image_sets/' \n",
        "width, height, nb_class = 768, 768, 3\n",
        "batch_size = 4 \n",
        "shuffle = True \n",
        "seed = 42 \n",
        "nb_epoch = 1\n",
        "loss='categorical_crossentropy'\n",
        "optimizer_cnn='adam'\n",
        "optimizer_sn = 'sgd'\n",
        "metrics='accuracy'\n",
        "target_size = (width, height)\n",
        "input_shape =  (width, height, nb_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfpHFSrIAaaR"
      },
      "source": [
        "### Função de Pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apevch6lfpGy"
      },
      "source": [
        "O conjunto de dados contém 15.000 imagens coloridas em 3 classes, com 5.000 imagens em cada classe. O conjunto de dados é dividido em 12.000 imagens de treinamento e 3.000 imagens de teste. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20j2yUBj21IG"
      },
      "source": [
        "def PreProcess(data_dir,target_size,batch_size,shuffle,seed):\n",
        "    \n",
        "    datagen_cnn = tf.keras.preprocessing.image.ImageDataGenerator(validation_split = 0.2)\n",
        "\n",
        "    train_cnn = datagen_cnn.flow_from_directory(data_dir,\n",
        "                                          class_mode = \"categorical\",\n",
        "                                          target_size = target_size,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          batch_size = batch_size, \n",
        "                                          shuffle = shuffle,\n",
        "                                          subset='training',\n",
        "                                          seed = seed)\n",
        "\n",
        "    validation_cnn = datagen_cnn.flow_from_directory(data_dir,\n",
        "                                          class_mode = \"categorical\",\n",
        "                                          target_size = target_size,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          batch_size = batch_size, \n",
        "                                          shuffle = shuffle,\n",
        "                                          subset='validation',\n",
        "                                          seed = seed)\n",
        "\n",
        "    datagen_sn = ImageDataGenerator(\n",
        "                                          rescale=1./255,\n",
        "                                          shear_range=0.2,\n",
        "                                          zoom_range=0.2,\n",
        "                                          horizontal_flip=True,\n",
        "                                          validation_split = 0.2\n",
        "                                          )\n",
        "\n",
        "    test_sn = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_sn = datagen_sn.flow_from_directory(\n",
        "                                          data_dir,\n",
        "                                          class_mode='categorical',\n",
        "                                          target_size = target_size,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = shuffle,\n",
        "                                          subset='training',\n",
        "                                          seed = seed)\n",
        "\n",
        "    validation_sn = datagen_sn.flow_from_directory(\n",
        "                                          data_dir,\n",
        "                                          class_mode='categorical',\n",
        "                                          target_size = target_size,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = shuffle,\n",
        "                                          subset='validation',\n",
        "                                          seed = seed)\n",
        "\n",
        "    return train_cnn, validation_cnn, train_sn, validation_sn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx2YrDwc1hhL"
      },
      "source": [
        "## Modelos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z1np-1Xb-ps"
      },
      "source": [
        "#### Criando bases convlucioinais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L-0PlRVcEwj"
      },
      "source": [
        "Pode-se ver que a saída de cada camada Conv2D e MaxPooling2D é um tensor 3D de forma (altura, largura, canais). As dimensões de largura e altura tendem a diminuir à medida que você se aprofunda na rede. O número de canais de saída para cada camada Conv2D é controlado pelo primeiro argumento (por exemplo, 32 ou 64). Normalmente, à medida que a largura e a altura diminuem, você pode (computacionalmente) adicionar mais canais de saída em cada camada Conv2D. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncVIknOucMT7"
      },
      "source": [
        "#### Adicionar camadas densas no topo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "400-15w5cJZV"
      },
      "source": [
        "Para completar o modelo, o último tensor de saída da base convolucional (de formato (4, 4, 64)) em uma ou mais camadas Densas para realizar a classificação. Camadas densas recebem vetores como entrada (que são 1D), enquanto a saída atual é um tensor 3D. Primeiro, você nivelará (ou desenrolará) a saída 3D em 1D e, em seguida, adicionará uma ou mais camadas Densas na parte superior. O conjunto tem 3 classes de saída, então você usa uma camada Densa final com 3 saídas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGynoYsq1jAc"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Owmt1_ci-m0"
      },
      "source": [
        "def Model_CNN():\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # Block 1\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv1', input_shape=(768, 768, 3)))\n",
        "  model.add(layers.Conv2D(32, (1, 1), activation='relu',name='block1_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block1_pool'))\n",
        "\n",
        "  # Block 2\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', name='block2_conv1'))\n",
        "  model.add(layers.Conv2D(64, (1, 1), activation='relu',name='block2_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block2_pool'))\n",
        "\n",
        "  # Block 3\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu',name='block3_conv1'))\n",
        "  model.add(layers.Conv2D(128, (1, 1), activation='relu',name='block3_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block3_pool'))\n",
        "\n",
        "  # Block 4\n",
        "  model.add(layers.Conv2D(256, (3, 3), activation='relu',name='block4_conv1'))\n",
        "  model.add(layers.Conv2D(256, (1, 1), activation='relu',name='block4_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block4_pool'))\n",
        "\n",
        "  # Block 5\n",
        "  model.add(layers.Conv2D(512, (3, 3), activation='relu',name='block5_conv1'))\n",
        "  model.add(layers.Conv2D(512, (1, 1), activation='relu',name='block5_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block5_pool'))\n",
        "\n",
        "  # Flatten\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hplcei4dcjXc"
      },
      "source": [
        "Como visto as (768, 768, 64) saídas foram achatadas em vetores de forma (387) antes de passar por duas camadas densas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK9aTjyu1m76"
      },
      "source": [
        "### VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVnHogyt6y9"
      },
      "source": [
        "def Model_VGG():\n",
        "  model = VGG16(include_top=False, input_shape=(768, 768, 3))\n",
        "\n",
        "  for layer in model.layers:\n",
        "   layer.trainable = False\n",
        "  \n",
        "  flat1 = Flatten()(model.layers[-1].output)\n",
        "  class1 = Dense(1024, activation='relu')(flat1)\n",
        "  class2 = Dense(512, activation='relu')(class1)\n",
        "  class3 = Dense(256, activation='relu')(class2)\n",
        "  class4 = Dense(128, activation='relu')(class3)\n",
        "  output = Dense(3, activation='softmax')(class4)\n",
        "\n",
        "  model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0omRck61pfq"
      },
      "source": [
        "### SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNGdcVZE0zfT"
      },
      "source": [
        "sq1x1 = \"squeeze1x1\"\n",
        "exp1x1 = \"expand1x1\"\n",
        "exp3x3 = \"expand3x3\"\n",
        "relu = \"relu_\"\n",
        "\n",
        "WEIGHTS_PATH = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5\"\n",
        "WEIGHTS_PATH_NO_TOP = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
        "    s_id = 'fire' + str(fire_id) + '/'\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = 3\n",
        "    \n",
        "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
        "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
        "\n",
        "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
        "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
        "\n",
        "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
        "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
        "\n",
        "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
        "    return x\n",
        "\n",
        "\n",
        "def Model_SqzNet(include_top=True, weights='imagenet',\n",
        "               input_tensor=None, input_shape=None,\n",
        "               pooling=None,\n",
        "               use_bn_on_input = False,\n",
        "               classes=1000):\n",
        "    \"\"\"Instantiates the SqueezeNet architecture.\n",
        "    \"\"\"\n",
        "        \n",
        "    if weights not in {'imagenet', None}:\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or `imagenet` '\n",
        "                         '(pre-training on ImageNet).')\n",
        "\n",
        "    if weights == 'imagenet' and classes != 1000:\n",
        "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=227,\n",
        "                                      min_size=48,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        raw_img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "    if use_bn_on_input:\n",
        "        img_input = BatchNormalization()(raw_img_input)\n",
        "    else:\n",
        "        img_input = raw_img_input\n",
        "\n",
        "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
        "    x = Activation('relu', name='relu_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
        "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
        "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
        "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
        "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
        "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
        "    \n",
        "    if include_top:\n",
        "    \n",
        "        x = Dropout(0.5, name='drop9')(x)\n",
        "\n",
        "        x = Convolution2D(classes, (1, 1), padding='valid', name='conv10')(x)\n",
        "        x = Activation('relu', name='relu_conv10')(x)\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Activation('softmax', name='loss')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "        elif pooling=='max':\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "        elif pooling==None:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unknown argument for 'pooling'=\" + pooling)\n",
        "\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    model = Model(inputs, x, name='squeezenet')\n",
        "\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                                    WEIGHTS_PATH,\n",
        "                                    cache_subdir='models')\n",
        "        else:\n",
        "            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                                    WEIGHTS_PATH_NO_TOP,\n",
        "                                    cache_subdir='models')\n",
        "            \n",
        "        model.load_weights(weights_path)\n",
        "        if K.backend() == 'theano':\n",
        "            layer_utils.convert_all_kernels_in_model(model)\n",
        "\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "\n",
        "            if K.backend() == 'tensorflow':\n",
        "                print('You are using the TensorFlow backend, yet you '\n",
        "                              'are using the Theano '\n",
        "                              'image data format convention '\n",
        "                              '(`image_data_format=\"channels_first\"`). '\n",
        "                              'For best performance, set '\n",
        "                              '`image_data_format=\"channels_last\"` in '\n",
        "                              'your Keras config '\n",
        "                              'at ~/.keras/keras.json.')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0jvliX61vJa"
      },
      "source": [
        "## Compilando o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNBZfMYKd5yF"
      },
      "source": [
        "train_cnn, validation_cnn, train_sn, validation_sn = PreProcess(data_dir,target_size,batch_size,shuffle,seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHY-ZdxkJ9aZ"
      },
      "source": [
        "#### Função para Compilar a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pulpavwaDk0m"
      },
      "source": [
        "def Compile_Model_CNN(loss, optimizer_cnn, metrics):\n",
        "  model_cnn = Model_CNN()\n",
        "  model_cnn.compile(loss=loss, optimizer=optimizer_cnn, metrics=[metrics])\n",
        "  return model_cnn\n",
        "\n",
        "def Compile_Model_VGG(loss, optimizer_cnn, metrics):\n",
        "  model_vgg = Model_VGG()\n",
        "  model_vgg.compile(loss=loss, optimizer=optimizer_cnn, metrics=[metrics])\n",
        "  return model_vgg\n",
        "\n",
        "def Compile_Model_SqzNet(input_shape, nb_class, loss, optimizer_sn, metrics):\n",
        "  model_sn = Model_SqzNet(input_shape=input_shape, weights = None, classes = nb_class)\n",
        "  sgd = SGD(lr=0.001, decay=0.0002, momentum=0.9, nesterov=True)\n",
        "  model_sn.compile(loss=loss, optimizer=optimizer_sn, metrics=[metrics])\n",
        "  return model_sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv_iQWYH1zET"
      },
      "source": [
        "### Compilando os modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnwXNtfXpWaZ"
      },
      "source": [
        "model_cnn = Compile_Model_CNN(loss, optimizer_cnn, metrics)\n",
        "model_vgg = Compile_Model_VGG(loss, optimizer_cnn, metrics)\n",
        "model_sn = Compile_Model_SqzNet(input_shape, nb_class, loss, optimizer_sn, metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3F4JZxd5wrJ"
      },
      "source": [
        "## Treinando o Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA_-LHzfIdVq"
      },
      "source": [
        "### Funções de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFxq8pHe6TDQ"
      },
      "source": [
        "def Train_Model_CNN(train_cnn, nb_epoch, validation_cnn):\n",
        "     model_cnn.fit(train_cnn, epochs = nb_epoch,  validation_data=validation_cnn)\n",
        "     return model_cnn\n",
        "\n",
        "def Train_Model_VGG(train_cnn, nb_epoch, validation_cnn):\n",
        "     model_vgg.fit(train_cnn, epochs = nb_epoch,  validation_data=validation_cnn)\n",
        "     return model_vgg\n",
        "\n",
        "def Train_Model_SqzNet(train_sn, nb_epoch, validation_sn):\n",
        "     model_sn.fit(train_sn, epochs = nb_epoch,  validation_data=validation_sn)\n",
        "     return model_sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaGlxicL50AP"
      },
      "source": [
        "### Treinando a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuAekSSPjm2_"
      },
      "source": [
        "history_cnn = Train_Model_CNN(train_cnn, nb_epoch, validation_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX0otG_AJE9y"
      },
      "source": [
        "### Treinando a VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZzY6TAlJYHL"
      },
      "source": [
        "history_vgg = Train_Model_VGG(train_cnn, nb_epoch, validation_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KcKjO9u53Er"
      },
      "source": [
        "### Treinando a SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgO4Ir23CzYz"
      },
      "source": [
        "history_sn = Train_Model_SqzNet(train_sn, nb_epoch, validation_sn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn_sw3kN55aE"
      },
      "source": [
        "## Avaliando o Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LZ7i2pf58i9"
      },
      "source": [
        "### Avaliando a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE9p2QFqmYWh"
      },
      "source": [
        "def Evaluate_Model_CNN():\n",
        "    predicted_cnn = model_cnn.predict(validation_cnn)\n",
        "\n",
        "    print(\"F1 Score--------------->\")\n",
        "    print(f1_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'micro'))\n",
        "    print(f1_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'macro'))\n",
        "    print(f1_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'weighted'))\n",
        "\n",
        "    print(\"Recall ------------>\")\n",
        "    print(recall_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'micro'))\n",
        "    print(recall_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'macro'))\n",
        "    print(recall_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'weighted'))\n",
        "    \n",
        "    print(\"Precision ------------>\")\n",
        "    print(precision_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'micro'))\n",
        "    print(precision_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'macro'))\n",
        "    print(precision_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ow9lzHcJn_9"
      },
      "source": [
        "### Avaliando a VGG "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euBAUFbiJyRM"
      },
      "source": [
        "def Evaluate_Model_VGG():\n",
        "    predicted_cnn = model_cnn.predict(validation_cnn)\n",
        "\n",
        "    print(\"F1 Score--------------->\")\n",
        "    print(f1_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'micro'))\n",
        "    print(f1_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'macro'))\n",
        "    print(f1_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'weighted'))\n",
        "\n",
        "    print(\"Recall ------------>\")\n",
        "    print(recall_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'micro'))\n",
        "    print(recall_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'macro'))\n",
        "    print(recall_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'weighted'))\n",
        "    \n",
        "    print(\"Precision ------------>\")\n",
        "    print(precision_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'micro'))\n",
        "    print(precision_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'macro'))\n",
        "    print(precision_score(validation_cnn.classes, np.argmax(predicted_cnn, axis = 1), average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqo1T2Do6Awa"
      },
      "source": [
        "### Avaliando a SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWv4I1I37UvQ"
      },
      "source": [
        "def Evaluate_Model_SqzNet():\n",
        "    predicted_sn = model_sn.predict(validation_sn)\n",
        "\n",
        "    print(\"F1 Score--------------->\")\n",
        "    print(f1_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'micro'))\n",
        "    print(f1_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'macro'))\n",
        "    print(f1_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'weighted'))\n",
        "\n",
        "    print(\"Recall ------------>\")\n",
        "    print(recall_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'micro'))\n",
        "    print(recall_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'macro'))\n",
        "    print(recall_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'weighted'))\n",
        "    \n",
        "    print(\"Precision ------------>\")\n",
        "    print(precision_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'micro'))\n",
        "    print(precision_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'macro'))\n",
        "    print(precision_score(validation_sn.classes, np.argmax(predicted_sn, axis = 1), average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIB-x4IIH_J9"
      },
      "source": [
        "## Salvando o Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WWr9oze4aDo"
      },
      "source": [
        "### Salvando a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipci4aMnvzP1"
      },
      "source": [
        "model_json = model_cnn.to_json()\n",
        "model_name = \"Modelo_CNN\"\n",
        "with open(model_name+\".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "model.save_weights(model_name+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsG_ln4-4efY"
      },
      "source": [
        "### Salvando a VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlXYzMuq4U5u"
      },
      "source": [
        "model_json = model_vgg.to_json()\n",
        "model_name = \"Modelo_VGG\"\n",
        "with open(model_name+\".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "model.save_weights(model_name+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xEp9YNY4gM4"
      },
      "source": [
        "### Salvando a SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwiH-rye-bts"
      },
      "source": [
        "model_json = model_sn.to_json()\n",
        "model_name = \"Modelo_SqueezeNet\"\n",
        "with open(model_name+\".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "model.save_weights(model_name+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1o-BSXF3m6k"
      },
      "source": [
        "## Execução "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDBk-PU93pD0"
      },
      "source": [
        "### Execução da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5x3cr4BY7oW"
      },
      "source": [
        "def Execute_Model_CNN():\n",
        "  \n",
        "  PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "  Compile_Model_CNN(loss, optimizer_cnn, metrics)\n",
        "  Train_Model_CNN(train_cnn, nb_epoch, validation_cnn)\n",
        "  Evaluate_Model_CNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTzPvRr84IZf"
      },
      "source": [
        "### Execução da VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE67syB69BK9"
      },
      "source": [
        "def Execute_Model_VGG():\n",
        " \n",
        "  PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "  Compile_Model_VGG(loss, optimizer_cnn, metrics)\n",
        "  Train_Model_VGG(train_cnn, nb_epoch, validation_cnn)\n",
        "  Evaluate_Model_VGG()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkh-knKd3uQP"
      },
      "source": [
        "### Execução da SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw6HWtO77Jhy"
      },
      "source": [
        "def Execute_Model_SqzNet():\n",
        "  PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "  Compile_Model_SqzNet(input_shape, nb_class, loss, optimizer_sn, metrics)\n",
        "  Train_Model_SqzNet(train_cnn, nb_epoch, validation_cnn)\n",
        "  Evaluate_Model_SqzNet()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
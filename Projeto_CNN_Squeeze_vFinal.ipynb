{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Projeto_CNN_Squeeze_vFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qsSxQJGtbqY0",
        "-mNIpyhd1b2b",
        "cJnvXAzwCWvu",
        "CybAIz5xCh3p",
        "1wzgfClBCqoV",
        "ApFgSRgHDIPU",
        "ruCjC2bMDPr-",
        "1Km_PHpFECl4",
        "XoMX8WVPD-ll",
        "NkL4_aCdD570",
        "l_8aLwEiDtqZ",
        "CxY-SCSQDxOw",
        "GvVQ4eNqDWID",
        "owghqPlyEF9E",
        "FQxas_T8EpAT",
        "HUwsp1iWEbBU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renanalencar/projeto-cnn-squeezenet/blob/main/Projeto_CNN_Squeeze_vFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TF_Ahcf1Wpk"
      },
      "source": [
        "# Projeto de Redes Neurais "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OTE95hcbO3U"
      },
      "source": [
        "## Referências: \n",
        "\n",
        "1. [Estudo sobre Câncer de Cólon utilizando a mesma base com modelod e CNN](https://www.kaggle.com/aayushrajput/lung-colon-cancer)\n",
        "2. [Modelo de SqueezeNet para reconhecimento de comida](https://www.kaggle.com/kmader/food-squeezenet) \n",
        "3. [SqueezeNet no Keras](https://codelabs.developers.google.com/codelabs/keras-flowers-squeezenet#6)\n",
        "4. [Entendendo Redes Convolucionais (CNNs) ](https://medium.com/neuronio-br/entendendo-redes-convolucionais-cnns-d10359f21184#:~:text=H%C3%A1%20muitas%20fun%C3%A7%C3%B5es%2C%20como%20sigmoid,quando%20comparada%20a%20outras%20fun%C3%A7%C3%B5es)\n",
        "5. [Uma introdução as redes neurais convolucionais utilizando o Keras](https://medium.com/data-hackers/uma-introdu%C3%A7%C3%A3o-as-redes-neurais-convolucionais-utilizando-o-keras-41ee8dcc033e)\n",
        "6. [Tutorial prático do Keras](https://cv-tricks.com/tensorflow-tutorial/keras/)\n",
        "7. [Métricas de avaliação de modelo](https://gabrielschade.github.io/2019/03/12/ml-classificacao-metricas.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsSxQJGtbqY0"
      },
      "source": [
        "## Erro na biblioteca keras_applications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iGEAqxx2l_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a06d37-f0fa-46f0-f712-5fdc19390017"
      },
      "source": [
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_applications in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mNIpyhd1b2b"
      },
      "source": [
        "## Importanto as bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-06T02:26:25.508587Z",
          "iopub.status.busy": "2021-01-06T02:26:25.507936Z",
          "iopub.status.idle": "2021-01-06T02:26:32.576008Z",
          "shell.execute_reply": "2021-01-06T02:26:32.576483Z"
        },
        "id": "iAve6DCL4JH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675f5c16-2f5f-49f2-bedf-e01eff415fa7"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "from keras import backend as K\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.models import Model, model_from_json\n",
        "from keras.layers import Dense,Flatten\n",
        "from keras.layers import Input, Activation, concatenate\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.layers import Dense,Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D,Conv2D\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import get_file\n",
        "from keras.utils import layer_utils\n",
        "from sklearn.metrics import f1_score,recall_score, precision_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('error')\n",
        "\n",
        "# from visual_callbacks import AccLossPlotter\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <_io.FileIO name=55 mode='wb' closefd=True>\n",
            "ResourceWarning: unclosed file <_io.FileIO name=55 mode='wb' closefd=True>\n",
            "Exception ignored in: <_io.FileIO name=56 mode='rb' closefd=True>\n",
            "ResourceWarning: unclosed file <_io.FileIO name=56 mode='rb' closefd=True>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83elpmo35iWc"
      },
      "source": [
        "## Pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F3v9sfRFmoc"
      },
      "source": [
        "O conjunto de dados contém 15.000 imagens coloridas em 3 classes, com 5.000 imagens em cada classe. O conjunto de dados é dividido em 12.000 imagens de treinamento e 3.000 imagens de teste. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJnvXAzwCWvu"
      },
      "source": [
        "### Variáveis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCmmex5F3MEb"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/lung_image_sets_teste/' # Diretório das imagems \n",
        "width, height, nb_class = 768, 768, 3 # Dimensões da entrada\n",
        "batch_size = 4 # Tamanho do lote para treinamento\n",
        "shuffle = True # Sortear as amostras \n",
        "seed = 42 # Número de sementes \n",
        "nb_epoch = 1 # Número de épocas \n",
        "loss='categorical_crossentropy' # Algoritmo de optmização das Redes Convolucionais\n",
        "optimizer_cnn='adam' # Algoritmo de optmização das Redes Convolucionais\n",
        "optimizer_sn = 'sgd' # Algoritmo de optmização da SqueezeNet\n",
        "metrics='accuracy' # Métrica utilizada \n",
        "target_size = (width, height)\n",
        "input_shape =  (width, height, nb_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CybAIz5xCh3p"
      },
      "source": [
        "### Pré-processamento das imagens "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbb2Fq_K09zX"
      },
      "source": [
        "def PreProcess(data_dir,target_size,batch_size,shuffle,seed):\n",
        "    \n",
        "    datagen_cnn = tf.keras.preprocessing.image.ImageDataGenerator(validation_split = 0.2)\n",
        "\n",
        "    train_cnn = datagen_cnn.flow_from_directory(data_dir,\n",
        "                                          class_mode = \"categorical\",\n",
        "                                          target_size = target_size,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          batch_size = batch_size, \n",
        "                                          shuffle = shuffle,\n",
        "                                          subset='training',\n",
        "                                          seed = seed)\n",
        "\n",
        "    validation_cnn = datagen_cnn.flow_from_directory(data_dir,\n",
        "                                          class_mode = \"categorical\",\n",
        "                                          target_size = target_size,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          batch_size = batch_size, \n",
        "                                          shuffle = shuffle,\n",
        "                                          subset='validation',\n",
        "                                          seed = seed)\n",
        "\n",
        "    return train_cnn, validation_cnn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSxtv_xt0kpB"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUACoLEQF3rz"
      },
      "source": [
        "A saída de cada camada Conv2D e MaxPooling2D é um tensor 3D de forma (altura, largura, canais). As dimensões de altura e largura tendem a diminuir à medida que você se aprofunda na rede. O número de canais de saída para cada camada Conv2D é controlado pelo primeiro argumento (por exemplo, 32 ou 64). Normalmente, à medida que a largura e a altura diminuem, você pode (computacionalmente) adicionar mais canais de saída em cada camada Conv2D."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wzgfClBCqoV"
      },
      "source": [
        "### Modelo de CNN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85ruv4xxGP6L"
      },
      "source": [
        "As linhas de código abaixo definir a base convolucional utilizando um padrão comum: uma pilha de Conv2D e MaxPooling2D camadas.\n",
        "\n",
        "Como entrada, um CNN assume tensores de forma (image_height, image_width, color_channels), ignorando o tamanho do lote. O color_channels se refere a (R, G, B). A CNN foi configuradapara processar entradas de forma (768, 768, 3), que é o formato de imagens lung_image_sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Owmt1_ci-m0"
      },
      "source": [
        "def Model_CNN():\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # BatchNormalization()(inp) testar o BatchNormalization\n",
        "  # Block 1\n",
        "  model.add(layers.BatchNormalization(input_shape=(768, 768, 3)))\n",
        "  # model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv1', input_shape=(768, 768, 3))) # Original\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv1'))\n",
        "  model.add(layers.Conv2D(32, (1, 1), activation='relu',name='block1_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block1_pool'))\n",
        "\n",
        "  # Block 2\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', name='block2_conv1'))\n",
        "  model.add(layers.Conv2D(64, (1, 1), activation='relu',name='block2_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block2_pool'))\n",
        "\n",
        "  # Block 3\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu',name='block3_conv1'))\n",
        "  model.add(layers.Conv2D(128, (1, 1), activation='relu',name='block3_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block3_pool'))\n",
        "\n",
        "  # Block 4\n",
        "  model.add(layers.Conv2D(256, (3, 3), activation='relu',name='block4_conv1'))\n",
        "  model.add(layers.Conv2D(256, (1, 1), activation='relu',name='block4_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block4_pool'))\n",
        "\n",
        "  # Block 5\n",
        "  model.add(layers.Conv2D(512, (3, 3), activation='relu',name='block5_conv1'))\n",
        "  model.add(layers.Conv2D(512, (1, 1), activation='relu',name='block5_conv2'))\n",
        "  model.add(layers.MaxPooling2D((2, 2),strides=(2, 2), name='block5_pool'))\n",
        "\n",
        "  # Flatten\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0quSU-YgGhgn"
      },
      "source": [
        "As (768, 768, 64) saídas serão achatadas em vetores de forma (387) antes de passar por duas camadas densas.\n",
        "\n",
        "Para completar o modelo, o último tensor de saída da base convolucional (de formato (4, 4, 64)) em uma ou mais camadas Densas para realizar a classificação. Camadas densas recebem vetores como entrada (que são 1D), enquanto a saída atual é um tensor 3D. Primeiro, você nivelará (ou desenrolará) a saída 3D em 1D e, em seguida, adicionará uma ou mais camadas Densas na parte superior. O conjunto tem 3 classes de saída, então você usa uma camada Densa final com 3 saídas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZKGXxHhCv0s"
      },
      "source": [
        "### Função para compilar o modelo da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pulpavwaDk0m"
      },
      "source": [
        "def Compile_Model_CNN(loss, optimizer_cnn, metrics):\n",
        "  model_cnn = Model_CNN()\n",
        "  model_cnn.compile(loss=loss, optimizer=optimizer_cnn, metrics=[metrics])\n",
        "  model_cnn.summary()\n",
        "  return model_cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7E0_4DC20D"
      },
      "source": [
        "### Função de treinamento da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJ7-eRj2bHg"
      },
      "source": [
        "def Train_Model_CNN(model_cnn, train_cnn, nb_epoch, validation_cnn):\n",
        "    history_cnn = model_cnn.fit(train_cnn, epochs = nb_epoch,  validation_data=validation_cnn)\n",
        "\n",
        "    # salvar modelo para posterior avaliação      \n",
        "    model_json = model_cnn.to_json()\n",
        "    model_name = \"Modelo_CNN\"\n",
        "    with open('/content/drive/MyDrive/'+model_name+\".json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    model_cnn.save_weights('/content/drive/MyDrive/'+model_name+'.h5')\n",
        "    \n",
        "    return history_cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj0ighk2C-D9"
      },
      "source": [
        "### Função para execução da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5x3cr4BY7oW"
      },
      "source": [
        "def Execute_Model_CNN():\n",
        "    train_cnn, validation_cnn = PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "    model_cnn = Compile_Model_CNN(loss, optimizer_cnn, metrics)\n",
        "    \n",
        "    history_cnn = Train_Model_CNN(model_cnn, train_cnn, nb_epoch, validation_cnn)\n",
        "    \n",
        "    # Plotar a curva de aprendizagem\n",
        "    history_dict = history_cnn.history\n",
        "    loss_values = history_dict['loss']\n",
        "    val_loss_values = history_dict['val_loss']\n",
        "    accuracy = history_dict['accuracy']\n",
        "    val_accuracy = history_dict['val_accuracy']\n",
        "    \n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "    # Plotar a Perdas vs Épocas\n",
        "    ax.plot(epochs, loss_values, 'bo', label='Perda de treinamento')\n",
        "    ax.plot(epochs, val_loss_values, 'b', label='Perda de validação')\n",
        "    ax.set_title('Perdas de Treinamento & Validação', fontsize=16)\n",
        "    ax.set_xlabel('Épocas', fontsize=16)\n",
        "    ax.set_ylabel('Perdas', fontsize=16)\n",
        "    ax.legend()\n",
        "\n",
        "    ax.figure.savefig('/content/drive/MyDrive/Curva_de_aprendizagem_CNN.png')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApFgSRgHDIPU"
      },
      "source": [
        "### Função para avaliação  da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTtM4Q_J3F4C"
      },
      "source": [
        "def Evaluate_Model_CNN():\n",
        "\n",
        "    train_cnn, validation_cnn = PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "\n",
        "    # Recuperando o modelo Treinado da CNN\n",
        "    with open('/content/drive/MyDrive/Modelo_CNN.json','r') as modelo:\n",
        "      json = modelo.read()\n",
        "    model_cnn = model_from_json(json)\n",
        "\n",
        "    # Recuperando os pesos do treinamento da CNN\n",
        "    model_cnn.load_weights('/content/drive/MyDrive/Modelo_CNN.h5') \n",
        "\n",
        "    # predicted_cnn = model_cnn.predict(validation_cnn)\n",
        "\n",
        "    # Y_pred = model_cnn.predict_generator(validation_cnn, steps=4500 // batchsize+1)\n",
        "    Y_pred = model_cnn.predict(validation_cnn)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    \n",
        "    print('\\nMatriz de Confusão')\n",
        "    print(confusion_matrix(validation_cnn.classes, y_pred),'\\n')\n",
        "\n",
        "    print(\" ---- F1 Score ----\")\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'micro'))\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'macro'))\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'weighted'),'\\n')\n",
        "\n",
        "    print(\" ---- Recall ----\")\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'micro'))\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'macro'))\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'weighted'),'\\n')\n",
        "    \n",
        "    print(\" ---- Precision ----\")\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'micro', zero_division=1))\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'macro', zero_division=1))\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'weighted', zero_division=1),'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BuGcrBcDLz7"
      },
      "source": [
        "### Chamada para execução da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pbF9sxmW3nIN",
        "outputId": "3debf1de-a95a-4126-b19b-fe46beb40e8c"
      },
      "source": [
        "Execute_Model_CNN()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 120 images belonging to 3 classes.\n",
            "Found 30 images belonging to 3 classes.\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_7 (Batch (None, 768, 768, 3)       12        \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 768, 768, 32)      896       \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 768, 768, 32)      1056      \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 384, 384, 32)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 382, 382, 64)      18496     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 382, 382, 64)      4160      \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 191, 191, 64)      0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 189, 189, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 189, 189, 128)     16512     \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 94, 94, 128)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 92, 92, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 92, 92, 256)       65792     \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 46, 46, 256)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 44, 44, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 44, 44, 512)       262656    \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 22, 22, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 247808)            0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 512)               126878208 \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 128,961,583\n",
            "Trainable params: 128,961,577\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n",
            "30/30 [==============================] - 7s 205ms/step - loss: 1.6478 - accuracy: 0.4073 - val_loss: 1.0887 - val_accuracy: 0.3333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruCjC2bMDPr-"
      },
      "source": [
        "### Chamada para avaliação da CNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zYrxTys3pPe",
        "outputId": "a54b488d-adf1-4114-8a05-8d7146334b4e"
      },
      "source": [
        "Evaluate_Model_CNN()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 120 images belonging to 3 classes.\n",
            "Found 30 images belonging to 3 classes.\n",
            "\n",
            "Matriz de Confusão\n",
            "[[ 0  0 10]\n",
            " [ 0  0 10]\n",
            " [ 0  0 10]] \n",
            "\n",
            " ---- F1 Score ----\n",
            "0.3333333333333333\n",
            "0.16666666666666666\n",
            "0.16666666666666666 \n",
            "\n",
            " ---- Recall ----\n",
            "0.3333333333333333\n",
            "0.3333333333333333\n",
            "0.3333333333333333 \n",
            "\n",
            " ---- Precision ----\n",
            "0.3333333333333333\n",
            "0.7777777777777778\n",
            "0.7777777777777778 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-_ob8w0szO"
      },
      "source": [
        "# VGG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Km_PHpFECl4"
      },
      "source": [
        "### Modelo VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVnHogyt6y9"
      },
      "source": [
        "def Model_VGG():\n",
        "  model = VGG16(include_top=False, input_shape=(768, 768, 3))\n",
        "\n",
        "  for layer in model.layers:\n",
        "   layer.trainable = False\n",
        "  \n",
        "  flat1 = Flatten()(model.layers[-1].output)\n",
        "  class1 = Dense(1024, activation='relu')(flat1)\n",
        "  class2 = Dense(512, activation='relu')(class1)\n",
        "  class3 = Dense(256, activation='relu')(class2)\n",
        "  class4 = Dense(128, activation='relu')(class3)\n",
        "  output = Dense(3, activation='softmax')(class4)\n",
        "\n",
        "  model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoMX8WVPD-ll"
      },
      "source": [
        "### Função para compilar o modelo da VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KdWBQjDlur9"
      },
      "source": [
        "def Compile_Model_VGG(loss, optimizer_cnn, metrics):\n",
        "  \n",
        "  model_vgg = Model_VGG()\n",
        "  model_vgg.compile(loss=loss, optimizer=optimizer_cnn, metrics=[metrics])\n",
        "  model_cnn.summary()\n",
        "\n",
        "  return model_vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkL4_aCdD570"
      },
      "source": [
        "### Função de treinamento da VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I67NDqig2fYw"
      },
      "source": [
        "def Train_Model_VGG(model_vgg, train_cnn, nb_epoch, validation_cnn):\n",
        "     \n",
        "     history_vgg = model_vgg.fit(train_cnn, epochs = nb_epoch,  validation_data=validation_cnn)\n",
        "\n",
        "     # salvar modelo para posterior avaliação      \n",
        "     model_json = model_vgg.to_json()\n",
        "     model_name = \"Modelo_VGG\"\n",
        "     with open('/content/drive/MyDrive/'+model_name+\".json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "     model_vgg.save_weights('/content/drive/MyDrive/'+model_name+'.h5')\n",
        "\n",
        "     return history_vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_8aLwEiDtqZ"
      },
      "source": [
        "### Função para execução da VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAp8s5ZQ2yQB"
      },
      "source": [
        "def Execute_Model_VGG():\n",
        "  \n",
        "    train_cnn, validation_cnn = PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "    model_vgg = Compile_Model_VGG(loss, optimizer_cnn, metrics)\n",
        "    model_vgg, history_vgg = Train_Model_VGG(model_vgg, train_cnn, nb_epoch, validation_cnn)\n",
        "\n",
        "    # Plotar a curva de aprendizagem\n",
        "    history_dict = history_vgg.history\n",
        "    loss_values = history_dict['loss']\n",
        "    val_loss_values = history_dict['val_loss']\n",
        "    accuracy = history_dict['accuracy']\n",
        "    val_accuracy = history_dict['val_accuracy']\n",
        "    \n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "    # Plotar a Perdas vs Épocas\n",
        "    ax.plot(epochs, loss_values, 'bo', label='Perda de treinamento')\n",
        "    ax.plot(epochs, val_loss_values, 'b', label='Perda de validação')\n",
        "    ax.set_title('Perdas de Treinamento & Validação', fontsize=16)\n",
        "    ax.set_xlabel('Épocas', fontsize=16)\n",
        "    ax.set_ylabel('Perdas', fontsize=16)\n",
        "    ax.legend()\n",
        "\n",
        "    ax.figure.savefig('/content/drive/MyDrive/Curva_de_aprendizagem_VGG.png')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxY-SCSQDxOw"
      },
      "source": [
        "### Função para avaliação  da VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Md9FUi21t0"
      },
      "source": [
        "def Evaluate_Model_VGG():\n",
        "\n",
        "    train_cnn, validation_cnn = PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "\n",
        "    # Recuperando o modelo Treinado da CNN\n",
        "    with open('/content/drive/MyDrive/Modelo_VGG.json','r') as modelo:\n",
        "      json = modelo.read()\n",
        "    model_vgg = model_from_json(json)\n",
        "\n",
        "    # Recuperando os pesos do treinamento da CNN\n",
        "    model_vgg.load_weights('/content/drive/MyDrive/Modelo_VGG.h5') \n",
        "\n",
        "    Y_pred = model_vgg.predict(validation_cnn)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    \n",
        "    print('\\nMatriz de Confusão')\n",
        "    print(confusion_matrix(validation_cnn.classes, y_pred),'\\n')\n",
        "\n",
        "    print(\" ---- F1 Score ----\")\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'micro'))\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'macro'))\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'weighted'),'\\n')\n",
        "\n",
        "    print(\" ---- Recall ----\")\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'micro'))\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'macro'))\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'weighted'),'\\n')\n",
        "    \n",
        "    print(\" ---- Precision ----\")\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'micro'))\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'macro'))\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'weighted', zero_division=1),'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-AlwkIODgbb"
      },
      "source": [
        "### Chamada para execução da VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpfw8xnX3h3x"
      },
      "source": [
        "Execute_Model_VGG()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvVQ4eNqDWID"
      },
      "source": [
        "### Chamada para avaliação da VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roJheMrn3sbX",
        "outputId": "01abe81f-e48c-414b-ad09-929f28cce482"
      },
      "source": [
        "Evaluate_Model_VGG()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 120 images belonging to 3 classes.\n",
            "Found 30 images belonging to 3 classes.\n",
            "\n",
            "Matriz de Confusão\n",
            "[[5 3 2]\n",
            " [4 3 3]\n",
            " [3 4 3]] \n",
            "\n",
            " ---- F1 Score ----\n",
            "0.36666666666666664\n",
            "0.36262626262626263\n",
            "0.3626262626262626 \n",
            "\n",
            " ---- Recall ----\n",
            "0.36666666666666664\n",
            "0.3666666666666667\n",
            "0.36666666666666664 \n",
            "\n",
            " ---- Precision ----\n",
            "0.36666666666666664\n",
            "0.36388888888888893\n",
            "0.36388888888888893 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTV_f7Ga0ukn"
      },
      "source": [
        "# SqueezeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owghqPlyEF9E"
      },
      "source": [
        "### Modelo SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNGdcVZE0zfT"
      },
      "source": [
        "sq1x1 = \"squeeze1x1\"\n",
        "exp1x1 = \"expand1x1\"\n",
        "exp3x3 = \"expand3x3\"\n",
        "relu = \"relu_\"\n",
        "\n",
        "WEIGHTS_PATH = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5\"\n",
        "WEIGHTS_PATH_NO_TOP = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
        "    s_id = 'fire' + str(fire_id) + '/'\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = 3\n",
        "    \n",
        "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
        "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
        "\n",
        "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
        "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
        "\n",
        "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
        "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
        "\n",
        "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
        "    return x\n",
        "\n",
        "\n",
        "def Model_SqzNet(include_top=True, weights='imagenet', \n",
        "               input_tensor=None, input_shape=None,\n",
        "               pooling=None,\n",
        "               use_bn_on_input = False,\n",
        "               classes=1000):\n",
        "    \"\"\"Instantiates the SqueezeNet architecture.\n",
        "    \"\"\"\n",
        "        \n",
        "    if weights not in {'imagenet', None}:\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or `imagenet` '\n",
        "                         '(pre-training on ImageNet).')\n",
        "\n",
        "    if weights == 'imagenet' and classes != 1000:\n",
        "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=227,\n",
        "                                      min_size=48,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        raw_img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "    if use_bn_on_input:\n",
        "        img_input = BatchNormalization()(raw_img_input)\n",
        "    else:\n",
        "        img_input = raw_img_input\n",
        "\n",
        "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
        "    x = Activation('relu', name='relu_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
        "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
        "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
        "\n",
        "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
        "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
        "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
        "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
        "    \n",
        "    if include_top:\n",
        "    \n",
        "        x = Dropout(0.5, name='drop9')(x)\n",
        "\n",
        "        x = Convolution2D(classes, (1, 1), padding='valid', name='conv10')(x)\n",
        "        x = Activation('relu', name='relu_conv10')(x)\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Activation('softmax', name='loss')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "        elif pooling=='max':\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "        elif pooling==None:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unknown argument for 'pooling'=\" + pooling)\n",
        "\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    model = Model(inputs, x, name='squeezenet')\n",
        "\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                                    WEIGHTS_PATH,\n",
        "                                    cache_subdir='models')\n",
        "        else:\n",
        "            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                                    WEIGHTS_PATH_NO_TOP,\n",
        "                                    cache_subdir='models')\n",
        "            \n",
        "        model.load_weights(weights_path)\n",
        "        if K.backend() == 'theano':\n",
        "            layer_utils.convert_all_kernels_in_model(model)\n",
        "\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "\n",
        "            if K.backend() == 'tensorflow':\n",
        "                print('You are using the TensorFlow backend, yet you '\n",
        "                              'are using the Theano '\n",
        "                              'image data format convention '\n",
        "                              '(`image_data_format=\"channels_first\"`). '\n",
        "                              'For best performance, set '\n",
        "                              '`image_data_format=\"channels_last\"` in '\n",
        "                              'your Keras config '\n",
        "                              'at ~/.keras/keras.json.')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQxas_T8EpAT"
      },
      "source": [
        "### Função para compilar o modelo da SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P445-sYxlwMu"
      },
      "source": [
        "def Compile_Model_SqzNet(input_shape, nb_class, loss, optimizer_sn, metrics):\n",
        " \n",
        "  model_sn = Model_SqzNet(input_shape=input_shape,  weights=None, classes = nb_class)\n",
        "  sgd = SGD(lr=0.001, decay=0.0002, momentum=0.9, nesterov=True)\n",
        "  model_sn.compile(loss=loss, optimizer=optimizer_sn, metrics=[metrics])\n",
        "  model_sn.summary()\n",
        "\n",
        "  return model_sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6co5yNMElFk"
      },
      "source": [
        "### Função para treinamento  da SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-cEMiO2_wQ"
      },
      "source": [
        "def Train_Model_SqzNet(model_sn, train_cnn, nb_epoch, validation_cnn):\n",
        "    \n",
        "    history_sn = model_sn.fit(train_cnn, epochs = nb_epoch,  validation_data=validation_cnn)\n",
        "\n",
        "    # salvar modelo para posterior avaliação      \n",
        "    model_json = model_sn.to_json()\n",
        "    model_name = \"Modelo_SqzNet\"\n",
        "    with open('/content/drive/MyDrive/'+model_name+\".json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "    model_sn.save_weights('/content/drive/MyDrive/'+model_name+'.h5')\n",
        "\n",
        "    return history_sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzr9rgQbEcNq"
      },
      "source": [
        "### Função para execuação  da SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAOX9uo3mFBm"
      },
      "source": [
        "def Execute_Model_SqzNet():\n",
        "    \n",
        "    train_cnn, validation_cnn = PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "    model_sn = Compile_Model_SqzNet(input_shape, nb_class, loss, optimizer_sn, metrics)\n",
        "    history_sn = Train_Model_SqzNet(model_sn, train_cnn, nb_epoch, validation_cnn)\n",
        "\n",
        "    # Plotar a curva de aprendizagem\n",
        "    history_dict = history_sn.history\n",
        "    loss_values = history_dict['loss']\n",
        "    val_loss_values = history_dict['val_loss']\n",
        "    accuracy = history_dict['accuracy']\n",
        "    val_accuracy = history_dict['val_accuracy']\n",
        "    \n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "    # Plotar a Perdas vs Épocas\n",
        "    ax.plot(epochs, loss_values, 'bo', label='Perda de treinamento')\n",
        "    ax.plot(epochs, val_loss_values, 'b', label='Perda de validação')\n",
        "    ax.set_title('Perdas de Treinamento & Validação', fontsize=16)\n",
        "    ax.set_xlabel('Épocas', fontsize=16)\n",
        "    ax.set_ylabel('Perdas', fontsize=16)\n",
        "    ax.legend()\n",
        "\n",
        "    ax.figure.savefig('/content/drive/MyDrive/Curva_de_aprendizagem_SqzNet.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUwsp1iWEbBU"
      },
      "source": [
        "### Função para avaliação  da SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5wa7dZ229Za"
      },
      "source": [
        "def Evaluate_Model_SqzNet():\n",
        "    \n",
        "    train_cnn, validation_cnn = PreProcess(data_dir,target_size,batch_size,shuffle,seed)\n",
        "\n",
        "    # Recuperando o modelo Treinado da CNN\n",
        "    with open('/content/drive/MyDrive/Modelo_SqzNet.json','r') as modelo:\n",
        "      json = modelo.read()\n",
        "    model_vgg = model_from_json(json)\n",
        "\n",
        "    # Recuperando os pesos do treinamento da CNN\n",
        "    model_vgg.load_weights('/content/drive/MyDrive/Modelo_SqzNet.h5') \n",
        "\n",
        "    Y_pred = model_vgg.predict(validation_cnn)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    \n",
        "    print('\\nMatriz de Confusão')\n",
        "    print(confusion_matrix(validation_cnn.classes, y_pred),'\\n')\n",
        "\n",
        "    print(\" ---- F1 Score ----\")\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'micro'))\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'macro'))\n",
        "    print(f1_score(validation_cnn.classes, y_pred, average = 'weighted'),'\\n')\n",
        "\n",
        "    print(\" ---- Recall ----\")\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'micro'))\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'macro'))\n",
        "    print(recall_score(validation_cnn.classes, y_pred, average = 'weighted'),'\\n')\n",
        "    \n",
        "    print(\" ---- Precision ----\")\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'micro', zero_division=1))\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'macro', zero_division=1))\n",
        "    print(precision_score(validation_cnn.classes, y_pred, average = 'weighted', zero_division=1),'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n67p6KcnETjh"
      },
      "source": [
        "### Chamada para execução da SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDJ6IHafy6d8"
      },
      "source": [
        "Execute_Model_SqzNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_pKf6nfEM7l"
      },
      "source": [
        "### Chamada para avaliação da SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q88gD9E3yoX"
      },
      "source": [
        "Evaluate_Model_SqzNet()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}